{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import re\n",
    "import random\n",
    "random.seed(1337)\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import model\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingFunction:\n",
    "\n",
    "    def __call__(self, df):\n",
    "        pass\n",
    "\n",
    "class PrecomputeFunction(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, new_col, dims, method=\"stack\"):\n",
    "        self.new_col = new_col\n",
    "        self.dims = dims\n",
    "        self.method=method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqLenExtractor(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, seq_col, new_col):\n",
    "        self.seq_col = seq_col\n",
    "        super().__init__(new_col, dims=(1,))\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        return df[self.seq_col].str.len()\n",
    "\n",
    "class KmerExtractor(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, seq_col, k, jump=False):\n",
    "        self.k = k\n",
    "        self.seq_col = seq_col\n",
    "        kmers += [''.join(i) for i in itertools.product([\"A\",\"C\",\"T\",\"G\"], repeat = self.k)]\n",
    "        self.n = len(kmers)\n",
    "        self.kmer_dict = {kmers[k]:k for k in range(self.n)}\n",
    "        self.jump = jump\n",
    "        super().__init__(new_col, dims=(self.n,))\n",
    "    \n",
    "    def extract(seq):\n",
    "        i = 0\n",
    "        arr = np.zeros(self.n)\n",
    "        while i < len(seq) - (self.k - 1):\n",
    "            arr[kmer_dict[seq[i:i+k]]] = arr[kmer_dict[seq[i:i+k]]] + 1\n",
    "            if self.jump:\n",
    "                i = i + self.k\n",
    "            else:\n",
    "                i += 1\n",
    "        return arr\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return df[self.seq_col].apply(extract)\n",
    "\n",
    "class GCContentExtractor(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, seq_col, new_col):\n",
    "        self.seq_col = seq_col\n",
    "        super().__init__(new_col, dims=(1,))\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        return (df[self.seq_col].str.count(\"G\") + \n",
    "                df[self.seq_col].str.count(\"C\"))/df[self.seq_col].str.len()\n",
    "    \n",
    "# Counts specific motifs (e.g. PolyA sites)\n",
    "class MotifExtractor(PrecomputeFunction):\n",
    "     \n",
    "    def __init__(self, seq_col, new_col, motifs):\n",
    "        self.seq_col = seq_col\n",
    "        self.n = motifs.len()\n",
    "        self.motifs = motifs\n",
    "        super().__init__(new_col, dims=(self.n,))\n",
    "    \n",
    "    def extract_motives(self, seq):\n",
    "        pass\n",
    "    # Use ahocorasick\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        return df[self.seq_col].apply(extract_motives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodererScore(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, noderer_df_aug, noderer_df_nonaug, new_col=\"noderer\",\n",
    "                utr_col=\"utr\", cds_col=\"cds\",\n",
    "                seq_col=\"sequence\", score_col=\"efficiency\"):\n",
    "        self.utr_col, self.cds_col = utr_col, cds_col\n",
    "        # replace U with T in Noderer dataframe\n",
    "        noderer_df_aug[seq_col] = noderer_df_aug[seq_col].str.replace(\"U\", \"T\")\n",
    "        noderer_df_nonaug[seq_col] = noderer_df_nonaug[seq_col].str.replace(\"U\", \"T\")\n",
    "        self.avg_score = noderer_df_nonaug[score_col].median()\n",
    "        # build dictionary\n",
    "        self.score_dict_aug = {k:v for k,v in zip(noderer_df_aug[seq_col], \n",
    "                                                  noderer_df_aug[score_col])}\n",
    "        self.score_dict_nonaug = {k:v for k,v in zip(noderer_df_nonaug[seq_col], \n",
    "                                              noderer_df_nonaug[score_col])}\n",
    "        super().__init__(new_col, dims=(1,))\n",
    "    \n",
    "    def score(self, tis):\n",
    "        score = self.score_dict_aug.get(tis)\n",
    "        if score is None:\n",
    "            score = self.score_dict_nonaug.get(tis)\n",
    "            if score is None:\n",
    "                score = self.avg_score\n",
    "        return score\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        tis = df[self.utr_col].str[-6:]\n",
    "        tis = tis.str.cat(df[self.cds_col].str[:5])\n",
    "        return tis.apply(self.score)\n",
    "\n",
    "class PrecomputeEmbeddings(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, new_col, model, layer_name, input_layer_names, \n",
    "                 generator_encoding_functions, node=0):\n",
    "        target_obj = model.get_layer(layer_name).get_output_at(node)\n",
    "        target = [target_obj]\n",
    "        self.check_fn = K.function([model.get_layer(x).input for x in input_layer_names], target)\n",
    "        self.generator_encoding_functions = generator_encoding_functions.copy()\n",
    "        super().__init__(new_col, dims=target_obj.shape, method=\"concat\")\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        generator = DataSequence(df, encoding_functions=self.generator_encoding_functions, \n",
    "                                 shuffle=False)\n",
    "        l = [self.check_fn(x)[0] for x in generator]\n",
    "        return functools.reduce(operator.concat, [np.vsplit(x, x.shape[0]) for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding functions\n",
    "class DataFrameExtractor(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, col, method=\"stack\"):\n",
    "        self.col = col\n",
    "        self.method = method\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        if self.method == \"stack\":\n",
    "            return np.stack(df[self.col], axis = 0)\n",
    "        else:\n",
    "            return np.concatenate(df[self.col], axis = 0)\n",
    "        \n",
    "class OneHotEncoder(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        max_len = len(max(df[self.col], key=len))\n",
    "        return np.stack([utils.encode_seq(seq, max_len) for seq in df[self.col]], axis = 0)\n",
    "\n",
    "class LibraryEncoder(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, col, n_libs=6):\n",
    "        self.col = col\n",
    "        self.n_libs=6\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        return utils.encode_experiment(df, col=self.col, n_libs=self.n_libs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "class DataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, precomputations=[], \n",
    "                 encoding_functions=[],\n",
    "                 output_encoding_fn=None,\n",
    "                 batch_size=128, shuffle=True):\n",
    "        self.df = df.copy()\n",
    "        self.encoding_functions = encoding_functions.copy()\n",
    "        self.output_encoding_fn = output_encoding_fn\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.batch_size = batch_size\n",
    "        for fn in precomputations:\n",
    "            print(\"Doing precomputation: \" + fn.new_col)\n",
    "            self.encoding_functions.append(DataFrameExtractor(fn.new_col, fn.method))\n",
    "            self.df[fn.new_col] = fn(df)\n",
    "        self.shuffle = shuffle\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_df = self.df.iloc[self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        # Feed input\n",
    "        inputs = [fn(batch_df) for fn in self.encoding_functions]\n",
    "        if self.output_encoding_fn is None:\n",
    "            return inputs\n",
    "        else:\n",
    "            return (inputs, self.output_encoding_fn(batch_df))\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indices after each epoch'\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(model, encoding_fn, mpra_df, endo_df, \n",
    "                        snv_df=None, ribo_df=None, ptr_df=None, \n",
    "                        do_test=False):\n",
    "    mpra_val = mpra_df[(mpra_df[\"set\"] == \"val\") & (mpra_df[\"library\"] == \"egfp_unmod_1\")]\n",
    "    endo_val = endo_df[endo_df[\"set\"] == \"val\"]\n",
    "    data_list = [mpra_val, endo_val]\n",
    "    if do_test:\n",
    "        mpra_test = mpra_df[(mpra_df[\"set\"] == \"test\") & (mpra_df[\"library\"] == \"egfp_unmod_1\")]\n",
    "        endo_test = endo_df[endo_df[\"set\"] == \"test\"]\n",
    "        data_list += [mpra_test, endo_test]\n",
    "    for df in data_list:\n",
    "        pred_gen = DataSequence(df, encoding_fn)\n",
    "        predictions = model.predict_generator(pred_gen)\n",
    "        print(utils.rSquared(predictions.reshape(-1), df[\"rl\"]))\n",
    "        utils.print_corrs(df[\"rl\"], predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/data_dict.pkl\", 'rb') as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "\n",
    "data_df = data_dict[\"data\"]\n",
    "snv_df = data_dict[\"snv\"]\n",
    "ptr_df = data_dict[\"ptr\"]\n",
    "\n",
    "with open(\"../Data/ribo_dict.pkl\", 'rb') as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "eichhorn_df = data_dict[\"eichhorn\"]\n",
    "eichhorn_df[\"log_load\"] = np.log(eichhorn_df[\"RPF_RPKM\"]/eichhorn_df[\"RNA_RPKM\"])\n",
    "\n",
    "with open(\"../Data/doudna_polysome_iso_sub.pkl\", 'rb') as handle:\n",
    "    doudna_df = pickle.load(handle)\n",
    "doudna_df[\"library\"] = \"egfp_unmod_1\"\n",
    "doudna_df = doudna_df.rename(index=str, columns={\"rl_mean\":\"rl\"})\n",
    "\n",
    "noderer_df_aug = pd.read_csv(\"../Data/TIS/tis_efficiencies_aug.tsv\", sep=\"\\t\")\n",
    "noderer_df_nonaug = pd.read_csv(\"../Data/TIS/tis_efficiencies_nonaug.tsv\", sep=\"\\t\")\n",
    "noderer_df_nonaug = noderer_df_nonaug.rename(index=str, columns={\"TIS_Sequence\":\"sequence\", \n",
    "                                             \"TIS_Efficiency\":\"efficiency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20845"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(doudna_df[\"3utr\"].str.contains(\"AATAAA\") | doudna_df[\"3utr\"].str.contains(\"AATTAA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doudna = doudna_df[[\"utr\", \"rl\", \"library\", \"set\", \"cds\", \"3utr\"]]\n",
    "train_doudna = train_doudna[train_doudna[\"set\"] == \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_doudna = doudna_df[[\"utr\", \"rl\", \"library\", \"set\", \"cds\", \"3utr\"]]\n",
    "val_doudna = val_doudna[val_doudna[\"set\"] == \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([data_df[(data_df[\"library\"] == \"egfp_unmod_1\") & (data_df[\"set\"] == \"train\")],\n",
    "                       train_doudna])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25135341877937317"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.evaluate_generator(base_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model(\"../Models/basic_model_scaled.h5\", custom_objects={'FrameSliceLayer': model.FrameSliceLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_fn = OneHotEncoder(\"utr\")\n",
    "library_fn = LibraryEncoder(\"library\")\n",
    "embeddings_fn = PrecomputeEmbeddings(\"embeddings\", base_model, \"fully_connected\", \n",
    "                                     [\"input_seq\", \"input_experiment\"], \n",
    "                                     generator_encoding_functions=[one_hot_fn, library_fn], \n",
    "                                     node=0)\n",
    "one_hot_fn_cds = OneHotEncoder(\"cds\")\n",
    "one_hot_fn_3utr = OneHotEncoder(\"3utr\")\n",
    "encoding_functions = [one_hot_fn, one_hot_fn_cds, one_hot_fn_3utr]\n",
    "precomputations = [embeddings_fn]\n",
    "output_encoding_fn = DataFrameExtractor(\"rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing precomputation: embeddings\n"
     ]
    }
   ],
   "source": [
    "generator = DataSequence(train_doudna, precomputations=precomputations, encoding_functions=encoding_functions,\n",
    "                        output_encoding_fn=output_encoding_fn,\n",
    "                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing precomputation: embeddings\n"
     ]
    }
   ],
   "source": [
    "val_gen = DataSequence(val_doudna, precomputations=precomputations, encoding_functions=encoding_functions,\n",
    "                      output_encoding_fn=output_encoding_fn,\n",
    "                      batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "reload(model_combined)\n",
    "utr5_conv = model_combined.framed_pooled_conv_model(n_conv_layers=5, \n",
    "                        kernel_size=[8,3,3,3,3], n_filters=32, dilations=[1, 2, 4, 8, 16],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"dense\", \n",
    "                        n_dense_layers=1, fc_neurons=[64], fc_drop_rate=0.2,\n",
    "                        prefix=\"5utr_\")\n",
    "utr3_conv = model_combined.pooled_conv_model(n_conv_layers=3, \n",
    "                        kernel_size=[3,3,3,3,3], n_filters=32, dilations=[1, 2, 4, 8, 16],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"dense\", \n",
    "                        single_output=False,\n",
    "                        n_dense_layers=1, fc_neurons=[32], fc_drop_rate=0.2,\n",
    "                        prefix=\"utr3_\")\n",
    "cds_conv = model_combined.pooled_conv_model(n_conv_layers=3, \n",
    "                        kernel_size=[3,3,3,3,3], n_filters=32, dilations=[1, 2, 4, 8, 16],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"dense\", \n",
    "                        single_output=False,\n",
    "                        n_dense_layers=1, fc_neurons=[32], fc_drop_rate=0.2,\n",
    "                        prefix=\"cds_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model_combined)\n",
    "embedding_input = model_combined.transfer_input((64 ,), \"input_embedding\")\n",
    "combined_model = model_combined.transfer_model(utr5_conv, cds_conv, utr3_conv,\n",
    "                  transfer_inputs=[embedding_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 198s - loss: 1.9357 - val_loss: 1.8717\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.87166, saving model to combined2.h5\n",
      "Epoch 2/30\n",
      " - 175s - loss: 1.8435 - val_loss: 1.8259\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.87166 to 1.82594, saving model to combined2.h5\n",
      "Epoch 3/30\n",
      " - 182s - loss: 1.7809 - val_loss: 1.8564\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.82594\n",
      "Epoch 4/30\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(\"combined2.h5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "combined_model.fit_generator(generator, epochs=30, verbose=2, validation_data=val_gen,\n",
    "                            callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=embeddings_fn(train_doudna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26415"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "reload(model_combined)\n",
    "utr5_conv = model_combined.framed_pooled_conv_model(n_conv_layers=5, \n",
    "                        kernel_size=[11,3,3,3,3], n_filters=32, dilations=[1, 2, 4, 8, 16],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"dense\", \n",
    "                        n_dense_layers=1, fc_neurons=[64], fc_drop_rate=0.2)\n",
    "utr3_conv = model_combined.kmer_linear_model(prefix=\"utr3\")\n",
    "cds_conv = model_combined.kmer_linear_model(prefix=\"cds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "reload(model_combined)\n",
    "utr5_conv = model_combined.framed_pooled_conv_model(n_conv_layers=5, \n",
    "                        kernel_size=[11,3,3,3,3], n_filters=128, dilations=[1, 2, 4, 8, 16],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"residual\", \n",
    "                        n_dense_layers=1, fc_neurons=[64], fc_drop_rate=0.2)\n",
    "utr3_conv = model_combined.kmer_linear_model(prefix=\"utr3\")\n",
    "cds_conv = model_combined.kmer_linear_model(prefix=\"cds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model_combined)\n",
    "combined_model = model_combined.combined_model_noshortcut(utr5_conv, cds_conv, utr3_conv,\n",
    "                  loss=model_combined.correlation_coefficient_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_fn = gen_encodingfn(kmer_gen(), kmer_gen(), col='utr', libcol=\"library\", n_libs=7,\n",
    "                  output_col=\"rl\", cds_col=\"cds\", utr3_col=\"3utr\")\n",
    "train_gen = BalancedDataSequence(train_doudna, data_df[(data_df[\"library\"] == \"egfp_unmod_1\") & (data_df[\"set\"] == \"train\")], encoding_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = DataSequence(val_doudna, encoding_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 302s - loss: 0.5566 - val_loss: 0.9903\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99033, saving model to combined2.h5\n",
      "Epoch 2/30\n",
      " - 314s - loss: 0.3522 - val_loss: 0.9907\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.99033\n",
      "Epoch 3/30\n",
      " - 309s - loss: 0.3343 - val_loss: 0.9670\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.99033 to 0.96705, saving model to combined2.h5\n",
      "Epoch 4/30\n",
      " - 315s - loss: 0.3262 - val_loss: 0.9567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96705 to 0.95671, saving model to combined2.h5\n",
      "Epoch 5/30\n",
      " - 314s - loss: 0.3255 - val_loss: 0.9669\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.95671\n",
      "Epoch 6/30\n",
      " - 300s - loss: 0.3175 - val_loss: 0.9602\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.95671\n",
      "Epoch 7/30\n",
      " - 309s - loss: 0.3142 - val_loss: 0.9640\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.95671\n",
      "Epoch 8/30\n",
      " - 308s - loss: 0.3098 - val_loss: 0.9535\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.95671 to 0.95354, saving model to combined2.h5\n",
      "Epoch 9/30\n",
      " - 294s - loss: 0.3048 - val_loss: 0.9690\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.95354\n",
      "Epoch 10/30\n",
      " - 311s - loss: 0.2987 - val_loss: 0.9652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.95354\n",
      "Epoch 11/30\n",
      " - 296s - loss: 0.2929 - val_loss: 0.9715\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.95354\n",
      "Epoch 12/30\n",
      " - 297s - loss: 0.2885 - val_loss: 0.9718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.95354\n",
      "Epoch 13/30\n",
      " - 293s - loss: 0.2863 - val_loss: 0.9733\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.95354\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd015ebc710>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(\"combined2.h5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "combined_model.fit_generator(train_gen, epochs=30, verbose=2, validation_data=val_gen,\n",
    "                            callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = load_model(\"combined2.h5\",  custom_objects={'FrameSliceLayer': model.FrameSliceLayer,\n",
    "                                                           \"correlation_coefficient_loss\":\n",
    "                                                            model_combined.correlation_coefficient_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.563278239517302\n",
      "Pearson: -0.485, p-val: 0.000, squared: 0.235, Spearman: -0.502, p-val: 0.000\n"
     ]
    }
   ],
   "source": [
    "doudna_train = doudna_df[doudna_df[\"set\"] == \"train\"]     \n",
    "pred_gen = DataSequence(doudna_train, encoding_fn)\n",
    "predictions = combined_model.predict_generator(pred_gen)\n",
    "print(utils.rSquared(predictions.reshape(-1), doudna_train[\"rl\"]))\n",
    "utils.print_corrs(doudna_train[\"rl\"], predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1214.2984697729644\n",
      "Pearson: -0.914, p-val: 0.000, squared: 0.835, Spearman: -0.889, p-val: 0.000\n",
      "-333.29901437130246\n",
      "Pearson: -0.197, p-val: 0.000, squared: 0.039, Spearman: -0.232, p-val: 0.000\n",
      "-819.6939772577493\n",
      "Pearson: -0.922, p-val: 0.000, squared: 0.850, Spearman: -0.901, p-val: 0.000\n",
      "-360.0016793565845\n",
      "Pearson: -0.213, p-val: 0.000, squared: 0.045, Spearman: -0.237, p-val: 0.000\n"
     ]
    }
   ],
   "source": [
    "compute_all_metrics(combined_model, encoding_fn, data_df, doudna_df, do_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df_exo, df_endo, encoding_fn, batch_size=64, extra_keys=[], shuffle=True):\n",
    "        self.df_exo, self.df_endo = df_exo, df_endo\n",
    "        self.encoding_fn = encoding_fn\n",
    "        self.indices_exo = np.arange(len(self.df_exo))\n",
    "        self.indices_endo = np.arange(len(self.df_endo))\n",
    "        self.extra_keys = extra_keys\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices_exo) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get some exogenous data\n",
    "        batch_df_exo = self.df_exo.iloc[\n",
    "            self.indices_exo[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        # Get a matching amount of endogenous data\n",
    "        idx_endo = idx\n",
    "        if (idx_endo + 1) * self.batch_size >= len(self.df_endo):\n",
    "            idx_endo = 0\n",
    "            np.random.shuffle(self.indices_endo)\n",
    "        batch_df_endo = self.df_endo.iloc[\n",
    "            self.indices_endo[idx_endo * self.batch_size:(idx_endo + 1) * self.batch_size]]\n",
    "        # Concatenate\n",
    "        batch_df = pd.concat([batch_df_exo, batch_df_endo])\n",
    "        # Prepare input data\n",
    "        encoded_data = self.encoding_fn(batch_df)\n",
    "        # Feed input\n",
    "        inputs = [encoded_data[\"seq\"], encoded_data[\"cds_seq\"], encoded_data[\"utr3_seq\"],\n",
    "                  encoded_data[\"seqtype\"]]\n",
    "        for key in self.extra_keys:\n",
    "            inputs.append(encoded_data[key])\n",
    "        if encoded_data.get(\"rl\") is None:\n",
    "            return inputs\n",
    "        else:\n",
    "            return (inputs, encoded_data[\"rl\"])\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indices after each epoch'\n",
    "        self.indices_exo = np.arange(len(self.df_exo))\n",
    "        self.indices_endo = np.arange(len(self.df_endo))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices_exo)\n",
    "            np.random.shuffle(self.indices_endo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
