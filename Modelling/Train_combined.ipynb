{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import re\n",
    "import random\n",
    "random.seed(1337)\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import ahocorasick\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "import model\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingFunction:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        pass\n",
    "\n",
    "class PrecomputeFunction(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, new_col, dims, method=\"stack\"):\n",
    "        self.new_col = new_col\n",
    "        self.dims = dims\n",
    "        self.method = method\n",
    "        super().__init__(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqLenExtractor(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, seq_col, new_col):\n",
    "        self.seq_col = seq_col\n",
    "        super().__init__(new_col, dims=(1,))\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        return df[self.seq_col].str.len()\n",
    "\n",
    "class KmerExtractor(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, seq_col, new_col, k, jump=False, divide_counts=True):\n",
    "        self.k = k\n",
    "        self.seq_col = seq_col\n",
    "        kmers = [''.join(i) for i in itertools.product([\"A\",\"C\",\"T\",\"G\"], repeat = self.k)]\n",
    "        self.n = len(kmers)\n",
    "        self.kmer_dict = {kmers[k]:k for k in range(self.n)}\n",
    "        self.jump = jump\n",
    "        self.divide_counts = divide_counts\n",
    "        super().__init__(new_col, dims=(self.n,))\n",
    "    \n",
    "    def extract(self, seq):\n",
    "        i = 0\n",
    "        arr = np.zeros(self.n)\n",
    "        while i < len(seq) - (self.k - 1):\n",
    "            arr[self.kmer_dict[seq[i:i+self.k]]] = arr[self.kmer_dict[seq[i:i+self.k]]] + 1\n",
    "            if self.jump:\n",
    "                i = i + self.k\n",
    "            else:\n",
    "                i += 1\n",
    "        if self.divide_counts:\n",
    "            arr/np.sum(arr)\n",
    "        return arr\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return df[self.seq_col].apply(self.extract)\n",
    "\n",
    "class KmerAtPosExtractor(PrecomputeFunction):\n",
    "\n",
    "    def __init__(self, seq_col, new_col, positions):\n",
    "        self.seq_col = seq_col\n",
    "        i = 0\n",
    "        self.pos_kmer_dict = {}\n",
    "        for start, stop in positions:\n",
    "            k = np.abs(stop - start)\n",
    "            kmers = [''.join(i) for i in itertools.product([\"A\",\"C\",\"T\",\"G\"], repeat = k)]\n",
    "            kmer_dict = {kmers[k]:k+i for k in range(len(kmers))}\n",
    "            i += len(kmers)\n",
    "            self.pos_kmer_dict[(start, stop)] = kmer_dict\n",
    "        self.n = i\n",
    "        self.pos = positions\n",
    "        super().__init__(new_col, dims=(self.n,))\n",
    "    \n",
    "    def extract(self, seq):\n",
    "        arr = np.zeros(self.n)\n",
    "        for interval in self.pos:\n",
    "            kmer_dict = self.pos_kmer_dict[interval]\n",
    "            start, stop = interval\n",
    "            arr[kmer_dict[seq[start:stop]]] = arr[kmer_dict[seq[start:stop]]] + 1\n",
    "        return arr\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return df[self.seq_col].apply(self.extract)\n",
    "    \n",
    "class GCContentExtractor(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, seq_col, new_col):\n",
    "        self.seq_col = seq_col\n",
    "        super().__init__(new_col, dims=(1,))\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        return (df[self.seq_col].str.count(\"G\") + \n",
    "                df[self.seq_col].str.count(\"C\"))/df[self.seq_col].str.len()\n",
    "    \n",
    "# Counts specific motifs (e.g. PolyA sites)\n",
    "class MotifExtractor(PrecomputeFunction):\n",
    "     \n",
    "    def __init__(self, seq_col, new_col, motifs):\n",
    "        self.seq_col = seq_col\n",
    "        self.n = len(motifs)\n",
    "        self.motifs = motifs\n",
    "        self.ahoAutomat = ahocorasick.Automaton()\n",
    "        for idx, key in enumerate(motifs):\n",
    "            self.ahoAutomat.add_word(key, idx)\n",
    "        self.ahoAutomat.make_automaton()\n",
    "        super().__init__(new_col, dims=(self.n,))\n",
    "    \n",
    "    def extract_motives(self, seq):\n",
    "        # Use ahocorasick\n",
    "        arr = np.zeros(self.n)\n",
    "        for end_index, idx in self.ahoAutomat.iter(seq):\n",
    "            arr[idx] += 1\n",
    "        return arr\n",
    "            \n",
    "    def __call__(self, df):\n",
    "        return df[self.seq_col].apply(self.extract_motives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodererScore(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, noderer_df_aug, noderer_df_nonaug, new_col=\"noderer\",\n",
    "                utr_col=\"utr\", cds_col=\"cds\",\n",
    "                seq_col=\"sequence\", score_col=\"efficiency\"):\n",
    "        self.utr_col, self.cds_col = utr_col, cds_col\n",
    "        # replace U with T in Noderer dataframe\n",
    "        noderer_df_aug[seq_col] = noderer_df_aug[seq_col].str.replace(\"U\", \"T\")\n",
    "        noderer_df_nonaug[seq_col] = noderer_df_nonaug[seq_col].str.replace(\"U\", \"T\")\n",
    "        self.avg_score = noderer_df_nonaug[score_col].median()\n",
    "        # build dictionary\n",
    "        self.score_dict_aug = {k:v for k,v in zip(noderer_df_aug[seq_col], \n",
    "                                                  noderer_df_aug[score_col])}\n",
    "        self.score_dict_nonaug = {k:v for k,v in zip(noderer_df_nonaug[seq_col], \n",
    "                                              noderer_df_nonaug[score_col])}\n",
    "        super().__init__(new_col, dims=(1,))\n",
    "    \n",
    "    def score(self, tis):\n",
    "        score = self.score_dict_aug.get(tis)\n",
    "        if score is None:\n",
    "            score = self.score_dict_nonaug.get(tis)\n",
    "            if score is None:\n",
    "                score = self.avg_score\n",
    "        return score\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        tis = df[self.utr_col].str[-6:]\n",
    "        tis = tis.str.cat(df[self.cds_col].str[:5])\n",
    "        return tis.apply(self.score)\n",
    "\n",
    "class PrecomputeEmbeddings(PrecomputeFunction):\n",
    "    \n",
    "    def __init__(self, new_col, model, layer_name, input_layer_names, \n",
    "                 generator_encoding_functions, node=0):\n",
    "        target_obj = model.get_layer(layer_name).get_output_at(node)\n",
    "        target = [target_obj]\n",
    "        self.check_fn = K.function([model.get_layer(x).input for x in input_layer_names], target)\n",
    "        self.generator_encoding_functions = generator_encoding_functions.copy()\n",
    "        super().__init__(new_col, dims=target_obj.shape, method=\"concat\")\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        generator = DataSequence(df, encoding_functions=self.generator_encoding_functions, \n",
    "                                 shuffle=False)\n",
    "        l = [self.check_fn(x)[0] for x in generator]\n",
    "        return functools.reduce(operator.concat, [np.vsplit(x, x.shape[0]) for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding functions\n",
    "class DataFrameExtractor(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, col, method=\"stack\"):\n",
    "        self.col = col\n",
    "        super().__init__(col)\n",
    "        self.method = method\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        if self.method == \"stack\":\n",
    "            return np.stack(df[self.col], axis = 0)\n",
    "        else:\n",
    "            return np.concatenate(df[self.col], axis = 0)\n",
    "        \n",
    "class OneHotEncoder(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "        super().__init__(col)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        max_len = len(max(df[self.col], key=len))\n",
    "        return np.stack([utils.encode_seq(seq, max_len) for seq in df[self.col]], axis = 0)\n",
    "\n",
    "class LibraryEncoder(EncodingFunction):\n",
    "    \n",
    "    def __init__(self, col, n_libs=6):\n",
    "        self.col = col\n",
    "        self.n_libs=6\n",
    "        super().__init__(col)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        return utils.encode_experiment(df, col=self.col, n_libs=self.n_libs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "class DataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df, precomputations=[], \n",
    "                 encoding_functions=[],\n",
    "                 input_order=None,\n",
    "                 output_encoding_fn=None,\n",
    "                 batch_size=128, shuffle=True):\n",
    "        self.df = df.copy()\n",
    "        self.encoding_functions = encoding_functions.copy()\n",
    "        self.output_encoding_fn = output_encoding_fn\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.batch_size = batch_size\n",
    "        for fn in precomputations:\n",
    "            print(\"Doing precomputation: \" + fn.new_col)\n",
    "            self.encoding_functions.append(DataFrameExtractor(fn.new_col, fn.method))\n",
    "            self.df[fn.new_col] = fn(df)\n",
    "        if input_order is not None:\n",
    "            fn_dict = {fn.name:fn for fn in self.encoding_functions}\n",
    "            self.encoding_functions = [fn_dict[name] for name in input_order]\n",
    "        self.shuffle = shuffle\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_df = self.df.iloc[self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        # Feed input\n",
    "        inputs = [fn(batch_df) for fn in self.encoding_functions]\n",
    "        if self.output_encoding_fn is None:\n",
    "            return inputs\n",
    "        else:\n",
    "            return (inputs, self.output_encoding_fn(batch_df))\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indices after each epoch'\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(model, encoding_fn, mpra_df, endo_df, \n",
    "                        snv_df=None, ribo_df=None, ptr_df=None, \n",
    "                        do_test=False):\n",
    "    mpra_val = mpra_df[(mpra_df[\"set\"] == \"val\") & (mpra_df[\"library\"] == \"egfp_unmod_1\")]\n",
    "    endo_val = endo_df[endo_df[\"set\"] == \"val\"]\n",
    "    data_list = [mpra_val, endo_val]\n",
    "    if do_test:\n",
    "        mpra_test = mpra_df[(mpra_df[\"set\"] == \"test\") & (mpra_df[\"library\"] == \"egfp_unmod_1\")]\n",
    "        endo_test = endo_df[endo_df[\"set\"] == \"test\"]\n",
    "        data_list += [mpra_test, endo_test]\n",
    "    for df in data_list:\n",
    "        pred_gen = DataSequence(df, encoding_fn)\n",
    "        predictions = model.predict_generator(pred_gen)\n",
    "        print(utils.rSquared(predictions.reshape(-1), df[\"rl\"]))\n",
    "        utils.print_corrs(df[\"rl\"], predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/data_dict.pkl\", 'rb') as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "\n",
    "data_df = data_dict[\"data\"]\n",
    "snv_df = data_dict[\"snv\"]\n",
    "ptr_df = data_dict[\"ptr\"]\n",
    "\n",
    "with open(\"../Data/ribo_dict.pkl\", 'rb') as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "eichhorn_df = data_dict[\"eichhorn\"]\n",
    "eichhorn_df[\"log_load\"] = np.log(eichhorn_df[\"RPF_RPKM\"]/eichhorn_df[\"RNA_RPKM\"])\n",
    "\n",
    "with open(\"../Data/doudna_polysome_iso_sub.pkl\", 'rb') as handle:\n",
    "    doudna_df = pickle.load(handle)\n",
    "doudna_df[\"library\"] = \"egfp_unmod_1\"\n",
    "doudna_df = doudna_df.rename(index=str, columns={\"rl_mean\":\"rl\"})\n",
    "\n",
    "noderer_df_aug = pd.read_csv(\"../Data/TIS/tis_efficiencies_aug.tsv\", sep=\"\\t\")\n",
    "noderer_df_nonaug = pd.read_csv(\"../Data/TIS/tis_efficiencies_nonaug.tsv\", sep=\"\\t\")\n",
    "noderer_df_nonaug = noderer_df_nonaug.rename(index=str, columns={\"TIS_Sequence\":\"sequence\", \n",
    "                                             \"TIS_Efficiency\":\"efficiency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doudna = doudna_df[[\"utr\", \"rl\", \"library\", \"set\", \"cds\", \"3utr\"]]\n",
    "train_doudna = train_doudna[train_doudna[\"set\"] == \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_doudna = doudna_df[[\"utr\", \"rl\", \"library\", \"set\", \"cds\", \"3utr\"]]\n",
    "val_doudna = val_doudna[val_doudna[\"set\"] == \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25135341877937317"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.evaluate_generator(base_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model(\"../Models/basic_model_scaled.h5\", custom_objects={'FrameSliceLayer': model.FrameSliceLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding functions\n",
    "one_hot_fn = OneHotEncoder(\"utr\")\n",
    "library_fn = LibraryEncoder(\"library\")\n",
    "one_hot_fn_cds = OneHotEncoder(\"cds\")\n",
    "one_hot_fn_3utr = OneHotEncoder(\"3utr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmer inputs\n",
    "# 5utr\n",
    "utr5_len = SeqLenExtractor(\"utr\", \"5utr_len\")\n",
    "utr5_gc = GCContentExtractor(\"utr\", \"5utr_gc\")\n",
    "kmer5utr = [utr5_len, utr5_gc]\n",
    "kmer_inputs_5utr = [model_combined.model_input(fn.dims, \"input_\"+ fn.new_col) for fn in kmer5utr]\n",
    "# cds\n",
    "cds_codon_bias = KmerExtractor(\"cds\", \"cds_codon_bias\", k=3, jump=True)\n",
    "cds_len = SeqLenExtractor(\"cds\", \"cds_len\")\n",
    "cds_gc = GCContentExtractor(\"cds\", \"cds_gc\")\n",
    "kmercds = [cds_codon_bias, cds_len, cds_gc]\n",
    "kmer_inputs_cds = [model_combined.model_input(fn.dims, \"input_\"+ fn.new_col) for fn in kmercds]\n",
    "# 3 utr\n",
    "utr3_len = SeqLenExtractor(\"3utr\", \"3utr_len\")\n",
    "utr3_gc = GCContentExtractor(\"3utr\", \"3utr_gc\")\n",
    "kmer3utr = [utr3_len, utr3_gc]\n",
    "kmer_inputs_3utr = [model_combined.model_input(fn.dims, \"input_\"+ fn.new_col) for fn in kmer3utr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer inputs\n",
    "embeddings_fn = PrecomputeEmbeddings(\"embeddings\", base_model, \"fully_connected\", \n",
    "                                     [\"input_seq\", \"input_experiment\"], \n",
    "                                     generator_encoding_functions=[one_hot_fn, library_fn], \n",
    "                                     node=0)\n",
    "embedding_input = model_combined.model_input((64 ,), \"input_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_functions = [one_hot_fn, one_hot_fn_cds, one_hot_fn_3utr]\n",
    "precomputations = kmer5utr + kmercds + kmer3utr\n",
    "precomputations = [embeddings_fn] + precomputations\n",
    "input_order=[\"utr\"] + [fn.new_col for fn in kmer5utr] + \\\n",
    "             [\"cds\"] + [fn.new_col for fn in kmercds] + \\\n",
    "            [\"3utr\"] + [fn.new_col for fn in kmer3utr]\n",
    "precomputations = [embeddings_fn] + precomputations\n",
    "input_order = input_order + [\"embeddings\"]\n",
    "output_encoding_fn = DataFrameExtractor(\"rl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing precomputation: 5utr_len\n",
      "Doing precomputation: 5utr_gc\n",
      "Doing precomputation: cds_codon_bias\n",
      "Doing precomputation: cds_len\n",
      "Doing precomputation: cds_gc\n",
      "Doing precomputation: 3utr_len\n",
      "Doing precomputation: 3utr_gc\n"
     ]
    }
   ],
   "source": [
    "train_gen = DataSequence(train_doudna, precomputations=precomputations, encoding_functions=encoding_functions,\n",
    "                        output_encoding_fn=output_encoding_fn,\n",
    "                        input_order=input_order,\n",
    "                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing precomputation: embeddings\n"
     ]
    }
   ],
   "source": [
    "val_gen = DataSequence(val_doudna, precomputations=precomputations, encoding_functions=encoding_functions,\n",
    "                      output_encoding_fn=output_encoding_fn,\n",
    "                      input_order=input_order,\n",
    "                      batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model_combined)\n",
    "utr_5kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "reload(model_combined)\n",
    "utr5_conv = model_combined.pooled_conv_model(n_conv_layers=1, \n",
    "                        kernel_size=[8], n_filters=64, dilations=[1],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"\",\n",
    "                        single_output=False,\n",
    "                        n_dense_layers=0, fc_neurons=[32], fc_drop_rate=0.2,\n",
    "                        prefix=\"5utr_\")\n",
    "cds_conv = model_combined.pooled_conv_model(n_conv_layers=1, \n",
    "                        kernel_size=[8], n_filters=32, dilations=[1],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"\", \n",
    "                        single_output=False,\n",
    "                        n_dense_layers=0, fc_neurons=[32], fc_drop_rate=0.2,\n",
    "                        prefix=\"cds_\")\n",
    "utr3_conv = model_combined.pooled_conv_model(n_conv_layers=1, \n",
    "                        kernel_size=[8], n_filters=32, dilations=[1],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"\", \n",
    "                        single_output=False,\n",
    "                        n_dense_layers=0, fc_neurons=[32], fc_drop_rate=0.2,\n",
    "                        prefix=\"utr3_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model_combined)\n",
    "utr5_model = combined_conv_kmer(utr5_conv, utr5_kmer, prefix=\"5utr\")\n",
    "cds_model = combined_conv_kmer(utr5_conv, utr5_kmer, prefix=\"cds\")\n",
    "_model = combined_conv_kmer(utr5_conv, utr5_kmer, prefix=\"cds\")\n",
    "combined_model = model_combined.transfer_model(utr5_conv, cds_conv, utr3_conv,\n",
    "                  transfer_inputs=[embedding_input], n_transfer_layers=0, \n",
    "                  n_combine_layers=1, combine_neurons=[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 197s - loss: 1.9373 - val_loss: 1.8187\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.81874, saving model to combined.h5\n",
      "Epoch 2/30\n",
      " - 184s - loss: 1.8137 - val_loss: 1.7925\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.81874 to 1.79253, saving model to combined.h5\n",
      "Epoch 3/30\n",
      " - 186s - loss: 1.7543 - val_loss: 1.8784\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.79253\n",
      "Epoch 4/30\n",
      " - 182s - loss: 1.7027 - val_loss: 2.0157\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.79253\n",
      "Epoch 5/30\n",
      " - 187s - loss: 1.6338 - val_loss: 1.7989\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.79253\n",
      "Epoch 6/30\n",
      " - 191s - loss: 1.5622 - val_loss: 1.8171\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.79253\n",
      "Epoch 7/30\n",
      " - 183s - loss: 1.4939 - val_loss: 1.7766\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.79253 to 1.77661, saving model to combined.h5\n",
      "Epoch 8/30\n",
      " - 190s - loss: 1.4067 - val_loss: 1.8533\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.77661\n",
      "Epoch 9/30\n",
      " - 183s - loss: 1.3518 - val_loss: 1.8817\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.77661\n",
      "Epoch 10/30\n",
      " - 186s - loss: 1.2766 - val_loss: 1.9081\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.77661\n",
      "Epoch 11/30\n",
      " - 192s - loss: 1.2044 - val_loss: 1.8840\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.77661\n",
      "Epoch 12/30\n",
      " - 186s - loss: 1.1309 - val_loss: 1.9126\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.77661\n",
      "Epoch 13/30\n",
      " - 182s - loss: 1.1094 - val_loss: 1.9314\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.77661\n",
      "Epoch 14/30\n",
      " - 192s - loss: 1.0518 - val_loss: 2.0128\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.77661\n",
      "Epoch 15/30\n",
      " - 189s - loss: 1.0041 - val_loss: 2.1865\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.77661\n",
      "Epoch 16/30\n",
      " - 189s - loss: 0.9552 - val_loss: 2.0495\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.77661\n",
      "Epoch 17/30\n",
      " - 186s - loss: 0.9227 - val_loss: 2.1450\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.77661\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8ad4574198>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint(\"combined.h5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "combined_model.fit_generator(generator, epochs=30, verbose=2, validation_data=val_gen,\n",
    "                            callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1153790229742846"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.evaluate_generator(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = load_model(\"combined2.h5\",  custom_objects={'FrameSliceLayer': model.FrameSliceLayer,\n",
    "                                                           \"correlation_coefficient_loss\":\n",
    "                                                            model_combined.correlation_coefficient_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing precomputation: embeddings\n",
      "0.5895364006084826\n",
      "Pearson: 0.792, p-val: 0.000, squared: 0.627, Spearman: 0.789, p-val: 0.000\n"
     ]
    }
   ],
   "source": [
    "generator = DataSequence(train_doudna, precomputations=precomputations, encoding_functions=encoding_functions,\n",
    "                        output_encoding_fn=output_encoding_fn,\n",
    "                        batch_size=32)\n",
    "predictions = combined_model.predict_generator(generator)\n",
    "print(utils.rSquared(predictions.reshape(-1), train_doudna[\"rl\"]))\n",
    "utils.print_corrs(train_doudna[\"rl\"], predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05038705961306167\n",
      "Pearson: 0.240, p-val: 0.000, squared: 0.058, Spearman: 0.231, p-val: 0.000\n"
     ]
    }
   ],
   "source": [
    "predictions = combined_model.predict_generator(val_gen)\n",
    "print(utils.rSquared(predictions.reshape(-1), val_doudna[\"rl\"]))\n",
    "utils.print_corrs(val_doudna[\"rl\"], predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing precomputation: embeddings\n",
      "0.06082193895161725\n",
      "Pearson: 0.269, p-val: 0.000, squared: 0.072, Spearman: 0.264, p-val: 0.000\n"
     ]
    }
   ],
   "source": [
    "test_doudna = doudna_df[[\"utr\", \"rl\", \"library\", \"set\", \"cds\", \"3utr\"]]\n",
    "test_doudna = test_doudna[test_doudna[\"set\"] == \"test\"]\n",
    "test_gen = DataSequence(test_doudna, precomputations=precomputations, encoding_functions=encoding_functions,\n",
    "                      output_encoding_fn=output_encoding_fn,\n",
    "                      batch_size=32, shuffle=False)\n",
    "predictions = combined_model.predict_generator(test_gen)\n",
    "print(utils.rSquared(predictions.reshape(-1), test_doudna[\"rl\"]))\n",
    "utils.print_corrs(test_doudna[\"rl\"], predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.get_layer(\"transfer_dense_0\").get_weights()[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model)\n",
    "reload(model_combined)\n",
    "utr5_conv = model_combined.framed_pooled_conv_model(n_conv_layers=5, \n",
    "                        kernel_size=[11,3,3,3,3], n_filters=128, dilations=[1, 2, 4, 8, 16],\n",
    "                        padding=\"same\", use_batchnorm=False,\n",
    "                        use_inception=False, skip_connections=\"residual\", \n",
    "                        n_dense_layers=1, fc_neurons=[64], fc_drop_rate=0.2)\n",
    "utr3_conv = model_combined.kmer_linear_model(prefix=\"utr3\")\n",
    "cds_conv = model_combined.kmer_linear_model(prefix=\"cds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(model_combined)\n",
    "combined_model = model_combined.combined_model_noshortcut(utr5_conv, cds_conv, utr3_conv,\n",
    "                  loss=model_combined.correlation_coefficient_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_fn = gen_encodingfn(kmer_gen(), kmer_gen(), col='utr', libcol=\"library\", n_libs=7,\n",
    "                  output_col=\"rl\", cds_col=\"cds\", utr3_col=\"3utr\")\n",
    "train_gen = BalancedDataSequence(train_doudna, data_df[(data_df[\"library\"] == \"egfp_unmod_1\") & (data_df[\"set\"] == \"train\")], encoding_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = DataSequence(val_doudna, encoding_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 302s - loss: 0.5566 - val_loss: 0.9903\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99033, saving model to combined2.h5\n",
      "Epoch 2/30\n",
      " - 314s - loss: 0.3522 - val_loss: 0.9907\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.99033\n",
      "Epoch 3/30\n",
      " - 309s - loss: 0.3343 - val_loss: 0.9670\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.99033 to 0.96705, saving model to combined2.h5\n",
      "Epoch 4/30\n",
      " - 315s - loss: 0.3262 - val_loss: 0.9567\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96705 to 0.95671, saving model to combined2.h5\n",
      "Epoch 5/30\n",
      " - 314s - loss: 0.3255 - val_loss: 0.9669\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.95671\n",
      "Epoch 6/30\n",
      " - 300s - loss: 0.3175 - val_loss: 0.9602\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.95671\n",
      "Epoch 7/30\n",
      " - 309s - loss: 0.3142 - val_loss: 0.9640\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.95671\n",
      "Epoch 8/30\n",
      " - 308s - loss: 0.3098 - val_loss: 0.9535\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.95671 to 0.95354, saving model to combined2.h5\n",
      "Epoch 9/30\n",
      " - 294s - loss: 0.3048 - val_loss: 0.9690\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.95354\n",
      "Epoch 10/30\n",
      " - 311s - loss: 0.2987 - val_loss: 0.9652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.95354\n",
      "Epoch 11/30\n",
      " - 296s - loss: 0.2929 - val_loss: 0.9715\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.95354\n",
      "Epoch 12/30\n",
      " - 297s - loss: 0.2885 - val_loss: 0.9718\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.95354\n",
      "Epoch 13/30\n",
      " - 293s - loss: 0.2863 - val_loss: 0.9733\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.95354\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd015ebc710>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(\"combined2.h5\", monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "combined_model.fit_generator(train_gen, epochs=30, verbose=2, validation_data=val_gen,\n",
    "                            callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = load_model(\"combined2.h5\",  custom_objects={'FrameSliceLayer': model.FrameSliceLayer,\n",
    "                                                           \"correlation_coefficient_loss\":\n",
    "                                                            model_combined.correlation_coefficient_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.563278239517302\n",
      "Pearson: -0.485, p-val: 0.000, squared: 0.235, Spearman: -0.502, p-val: 0.000\n"
     ]
    }
   ],
   "source": [
    "doudna_train = doudna_df[doudna_df[\"set\"] == \"train\"]     \n",
    "pred_gen = DataSequence(doudna_train, encoding_fn)\n",
    "predictions = combined_model.predict_generator(pred_gen)\n",
    "print(utils.rSquared(predictions.reshape(-1), doudna_train[\"rl\"]))\n",
    "utils.print_corrs(doudna_train[\"rl\"], predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1214.2984697729644\n",
      "Pearson: -0.914, p-val: 0.000, squared: 0.835, Spearman: -0.889, p-val: 0.000\n",
      "-333.29901437130246\n",
      "Pearson: -0.197, p-val: 0.000, squared: 0.039, Spearman: -0.232, p-val: 0.000\n",
      "-819.6939772577493\n",
      "Pearson: -0.922, p-val: 0.000, squared: 0.850, Spearman: -0.901, p-val: 0.000\n",
      "-360.0016793565845\n",
      "Pearson: -0.213, p-val: 0.000, squared: 0.045, Spearman: -0.237, p-val: 0.000\n"
     ]
    }
   ],
   "source": [
    "compute_all_metrics(combined_model, encoding_fn, data_df, doudna_df, do_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedDataSequence(Sequence):\n",
    "    \n",
    "    def __init__(self, df_exo, df_endo, encoding_fn, batch_size=64, extra_keys=[], shuffle=True):\n",
    "        self.df_exo, self.df_endo = df_exo, df_endo\n",
    "        self.encoding_fn = encoding_fn\n",
    "        self.indices_exo = np.arange(len(self.df_exo))\n",
    "        self.indices_endo = np.arange(len(self.df_endo))\n",
    "        self.extra_keys = extra_keys\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices_exo) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get some exogenous data\n",
    "        batch_df_exo = self.df_exo.iloc[\n",
    "            self.indices_exo[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        # Get a matching amount of endogenous data\n",
    "        idx_endo = idx\n",
    "        if (idx_endo + 1) * self.batch_size >= len(self.df_endo):\n",
    "            idx_endo = 0\n",
    "            np.random.shuffle(self.indices_endo)\n",
    "        batch_df_endo = self.df_endo.iloc[\n",
    "            self.indices_endo[idx_endo * self.batch_size:(idx_endo + 1) * self.batch_size]]\n",
    "        # Concatenate\n",
    "        batch_df = pd.concat([batch_df_exo, batch_df_endo])\n",
    "        # Prepare input data\n",
    "        encoded_data = self.encoding_fn(batch_df)\n",
    "        # Feed input\n",
    "        inputs = [encoded_data[\"seq\"], encoded_data[\"cds_seq\"], encoded_data[\"utr3_seq\"],\n",
    "                  encoded_data[\"seqtype\"]]\n",
    "        for key in self.extra_keys:\n",
    "            inputs.append(encoded_data[key])\n",
    "        if encoded_data.get(\"rl\") is None:\n",
    "            return inputs\n",
    "        else:\n",
    "            return (inputs, encoded_data[\"rl\"])\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indices after each epoch'\n",
    "        self.indices_exo = np.arange(len(self.df_exo))\n",
    "        self.indices_endo = np.arange(len(self.df_endo))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices_exo)\n",
    "            np.random.shuffle(self.indices_endo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
