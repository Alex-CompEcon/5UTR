{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_and_sort(path):\n",
    "    # print(path)\n",
    "    df = pd.read_csv(path)\n",
    "    # drop the suffix\n",
    "    df[\"utr\"] = df[\"utr\"].str[:50]\n",
    "    # reorder\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)  # drop first column\n",
    "    if 'total_reads' in df:\n",
    "        df.sort_values(by=['total_reads'], inplace=True, ascending=False)\n",
    "    else:\n",
    "        df.sort_values(by=['total'], inplace=True, ascending=False)\n",
    "    df.reset_index(inplace=True, drop=True)  # necessary as sorting creates an extra index\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We begin by reading in all the raw data, sorting it and removing suffixes not part of the UTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['egfp_unmod_1', 'egfp_pseudo_2', 'egfp_m1pseudo_1', 'egfp_m1pseudo_2', 'mcherry_1', 'mcherry_2', 'designed_library', 'egfp_unmod_2', 'egfp_pseudo_1'])\n",
      "dict_keys(['egfp_unmod_1', 'mcherry_1', 'mcherry_2', 'designed_library', 'egfp_unmod_2'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Read in all Data \"\"\"\n",
    "path = \"../Data/RawData/\"\n",
    "files = [os.path.join(path,file) for file in os.listdir(path) if file.startswith(\"GSM\")]\n",
    "df_list = {re.search(\"_(.*)\\.\",file).group(1):import_data_and_sort(file) for file in files}\n",
    "print(df_list.keys())\n",
    "# Remove the nonstandard chemistries\n",
    "entriesToRemove = ['egfp_pseudo_1', 'egfp_pseudo_2', 'egfp_m1pseudo_1', 'egfp_m1pseudo_2']\n",
    "for k in entriesToRemove:\n",
    "    df_list.pop(k, None)\n",
    "print(df_list.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We subset the data using the same cutoffs as in the sample code (except for the genetic algorithm data, where I impose a cutoff of minimum 200 reads (as I couldnt find which value is actually used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the random mpra data\n",
    "keys = ['egfp_unmod_1', 'egfp_unmod_2', 'mcherry_1', 'mcherry_2']\n",
    "cuts = [280000, 300000, 180000, 170000]\n",
    "for key, cutoff in zip(keys, cuts):\n",
    "    df_list[key] = df_list[key].iloc[:cutoff].copy()\n",
    "\n",
    "# Subsetting the human data    \n",
    "human = df_list[\"designed_library\"][(df_list[\"designed_library\"]['library'] == 'human_utrs') | \n",
    "                                    (df_list[\"designed_library\"]['library'] == 'snv')]\n",
    "human = human.sort_values('total', ascending=False).reset_index(drop=True)\n",
    "human = human.iloc[:25000].copy()\n",
    "\n",
    "# Subsetting the genetic algorithm data\n",
    "GA_types = ['step_random_to_best_allow_uatg',\n",
    " 'step_random_to_best_no_uatgs',\n",
    " 'step_worst_to_best_allow_uatg',\n",
    " 'step_worst_to_best_no_uatg',\n",
    " 'target_allow_uaug_allow_stop',\n",
    " 'target_no_uaug_allow_stop',\n",
    " 'target_no_uaug_no_stop']\n",
    "GA = df_list['designed_library'][df_list['designed_library'][\"library\"].isin(GA_types)]\n",
    "GA = GA.iloc[:sum(GA[\"total\"] >= 200)].copy()\n",
    "\n",
    "df_list.pop(\"designed_library\", None)\n",
    "df_list[\"human\"] = human\n",
    "df_list[\"ga\"] = GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We reduce to the needed columns (utr and rl) and add a library column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_list.items():\n",
    "    df = df.filter(regex=(\"rl|utr\"))\n",
    "    df[\"library\"] = key\n",
    "    df_list[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We prepare the test (20k), val (20k) and train (rest) split for all the sets except ga (only for training) and human (only for validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_list.items():\n",
    "    df[\"set\"] = \"\"\n",
    "    if key == \"human\":\n",
    "        df[\"set\"] = \"val\"\n",
    "    elif key == \"ga\":\n",
    "        df[\"set\"] = \"train\"\n",
    "    else:\n",
    "        df.loc[:20000, \"set\"] = \"test\"\n",
    "        df.loc[20000:40000, \"set\"] = \"val\"\n",
    "        df.loc[40000:, \"set\"] = \"train\"\n",
    "    df_list[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We combine the data into one large frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list.values())    \n",
    "combined_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We add the TIS context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dict = {\"egfp_unmod_1\": 'ATGGGCGAATTAAGTAAGGGCGAGGAGCTGTTCACCGGGGTG', \n",
    "                \"egfp_unmod_2\": 'ATGGGCGAATTAAGTAAGGGCGAGGAGCTGTTCACCGGGGTG', \n",
    "                \"mcherry_1\": \"ATGCCTCCCGAGAAGAAGATCAAGAGCGTGAGCAAGGGCGA\", \n",
    "                \"mcherry_2\": \"ATGCCTCCCGAGAAGAAGATCAAGAGCGTGAGCAAGGGCGA\", \n",
    "                \"ga\": 'ATGGGCGAATTAAGTAAGGGCGAGGAGCTGTTCACCGGGGTG', \n",
    "                \"human\": 'ATGGGCGAATTAAGTAAGGGCGAGGAGCTGTTCACCGGGGTG'}\n",
    "combined_df[\"context\"] = [context_dict[x] for x in combined_df[\"library\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"cds\"] = \"XXX\"\n",
    "combined_df[\"3utr\"] = \"XXX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We collect the snv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_df = pd.read_csv(\"../Data/SNV/snv_phenotype_log_diff.csv\")\n",
    "snv_df.drop(['Unnamed: 0'], axis=1, inplace=True)  # drop first column\n",
    "snv_df = snv_df[snv_df['obs_diff'] != 0.0]\n",
    "snv_df = snv_df[snv_df['total'] >= 620]\n",
    "snv_df[\"cds\"] = 'XXX'\n",
    "snv_df[\"3utr\"] = \"XXX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We collect the PTR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# We get the PTR data\n",
    "ptr_df = pd.read_csv(\"../Data/PTR/ptr.tsv\", sep='\\t')\n",
    "# We average over all tissues\n",
    "ptr_vals = ptr_df.select(lambda col: col.endswith('PTR'), axis=1).apply(pd.to_numeric, errors='coerce').mean(axis=1)\n",
    "ptr_vals_df = pd.DataFrame({\"GeneName\":ptr_df[\"GeneName\"], \"PTR\":ptr_vals})\n",
    "\n",
    "# We get the sequences\n",
    "seq_df = pd.read_csv(\"../Data/PTR/seq.tsv\", sep='\\t')\n",
    "\n",
    "# We combine\n",
    "combined_df_ptr = seq_df[[\"GeneName\",\"UTR5_Sequence\", \"CDS_Sequence\", \"UTR3_Sequence\"]].merge(ptr_vals_df)\n",
    "combined_df_ptr = combined_df_ptr.rename(index=str, columns={\"UTR5_Sequence\": \"utr\", \n",
    "                                                            \"CDS_Sequence\":\"cds\",\n",
    "                                                            \"UTR3_Sequence\":\"3utr\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the data in a dict and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"data\":combined_df, \"snv\":snv_df, \"ptr\":combined_df_ptr}\n",
    "with open(\"../Data/data_dict.pkl\", 'wb') as handle:\n",
    "    pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the Riboseq data and combine it with sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the sequences\n",
    "seq_df = pd.read_csv(\"../Data/PTR/seq.tsv\", sep='\\t')\n",
    "seq_df = seq_df.rename(index=str, columns={\"UTR5_Sequence\": \"utr\",\n",
    "                                          \"CDS_Sequence\":\"cds\",\n",
    "                                          \"UTR3_Sequence\":\"3utr\"})\n",
    "\n",
    "#We get the andreev riboseq data and combine\n",
    "andreev_df = pd.read_csv(\"../Data/RiboSeq/andreev_counts.tsv\", sep='\\t', decimal=\",\")\n",
    "andreev_df = andreev_df.rename(index=str, columns={\"Gene name\": \"GeneName\", \n",
    "                                                   \"Riboseq control reads, coding\": \"rpf\",\n",
    "                                                   \"RNAseq control, (normalised)\": \"rnaseq_norm\"})\n",
    "\n",
    "andreev_merged = andreev_df[[\"GeneName\",\"rpf\",\"rnaseq_norm\"]].merge(seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the xtail pcr3 riboseq data and combine\n",
    "pcr3_df = pd.read_csv(\"../Data/RiboSeq/xtail_counts_pcr3.tsv\", sep='\\t', decimal=\",\")\n",
    "pcr3_df = pcr3_df.rename(index=str, columns={\"Ensembl_ID\": \"EnsemblGeneID\"})\n",
    "pcr3_df = pcr3_df.rename(columns=lambda x: re.sub('.1$','_normalized',x))\n",
    "\n",
    "pcr3_merged = pcr3_df.merge(seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the Eichhorn hek293 riboseq data and combine\n",
    "eichhorn_df = pd.read_csv(\"../Data/RiboSeq/Eichhorn_GSE60426_MockHEK293T.tsv\", sep='\\t', decimal=\".\")\n",
    "\n",
    "eichhorn_merged = eichhorn_df.merge(seq_df)\n",
    "eichhorn_merged = eichhorn_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ribo_dict = {\"andreev\":andreev_merged, \"pcr3\":pcr3_merged, \"eichhorn\": eichhorn_merged}\n",
    "with open(\"../Data/ribo_dict.pkl\", 'wb') as handle:\n",
    "    pickle.dump(ribo_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare more PTR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the eraslan PTR data\n",
    "eraslan_ptr_df = pd.read_csv(\"../Data/PTR/ptr.tsv\", sep='\\t')\n",
    "\n",
    "# We get the sequences\n",
    "seq_df = pd.read_csv(\"../Data/PTR/seq.tsv\", sep='\\t')\n",
    "seq_df = seq_df.rename(index=str, columns={\"UTR5_Sequence\": \"utr\"})\n",
    "\n",
    "# We combine\n",
    "combined_eraslan = seq_df.merge(eraslan_ptr_df, on=\"GeneName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the Zheng PTR data\n",
    "zheng_ptr_df = pd.read_csv(\"../Data/PTR/Zheng_ptr.tsv\", sep='\\t', decimal=\",\")\n",
    "\n",
    "zheng_ptr_df = zheng_ptr_df.rename(index=str, columns={\"GeneSymbol\": \"GeneName\"})\n",
    "combined_zheng = seq_df.merge(zheng_ptr_df, on=\"GeneName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the wilhelm PTR data\n",
    "wilhelm_ptr_df = pd.read_csv(\"../Data/PTR/wilhelm_ptr.tsv\", sep='\\t', decimal=\",\")\n",
    "wilhelm_ptr_df = wilhelm_ptr_df.dropna()\n",
    "\n",
    "wilhelm_ptr_df = wilhelm_ptr_df.rename(index=str, columns={\"Accessions\": \"EnsemblGeneID\",\n",
    "                                                          \"protein/mRNA ratio\": \"ptr\"})\n",
    "combined_wilhelm = seq_df.merge(wilhelm_ptr_df, on=\"EnsemblGeneID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr_dict = {\"eraslan\":combined_eraslan, \"zheng\":combined_zheng, \"wilhelm\": combined_wilhelm}\n",
    "with open(\"../Data/ptr_dict.pkl\", 'wb') as handle:\n",
    "    pickle.dump(ptr_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the polysome profiling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  import sys\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if __name__ == '__main__':\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "doudna_df = pd.read_csv(\"../Data/TrIP-Seq/doudna_polysome_tripseq_gene_tpm_ensembl_v75.csv\")\n",
    "doudna_df = doudna_df.rename(index=str, columns={\"gene_id\": \"EnsemblGeneID\",\n",
    "                                                 \"isoform_id\": \"EnsemblTranscriptID\",\n",
    "                                                      \"gene_name\": \"GeneName\"})\n",
    "\n",
    "\n",
    "# replicate 1\n",
    "fractions_1 = doudna_df.select(lambda col: re.match(\"poly._1|80S_1\", col), axis=1)\n",
    "doudna_df[\"count_1\"] = fractions_1.sum(axis=1)\n",
    "doudna_df[\"rl_1\"] = np.sum(np.array(fractions_1) * np.arange(1,9), axis=1)/np.sum(np.array(fractions_1),axis=1)\n",
    "# replicate 2\n",
    "fractions_2 = doudna_df.select(lambda col: re.match(\"poly._2|80S_2\", col), axis=1)\n",
    "doudna_df[\"count_2\"] = fractions_2.sum(axis=1)\n",
    "doudna_df[\"rl_2\"] = np.sum(np.array(fractions_2) * np.arange(1,9), axis=1)/np.sum(np.array(fractions_2),axis=1)\n",
    "# replicate mean\n",
    "fractions = (np.array(fractions_1) + np.array(fractions_2))/2\n",
    "doudna_df[\"count_mean\"] = np.sum(fractions, axis=1)\n",
    "doudna_df[\"rl_mean\"] = np.sum(fractions * np.arange(1,9), axis=1)/np.sum(fractions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df = pd.read_csv(\"../Data/PTR/seq.tsv\", sep='\\t')\n",
    "seq_df = seq_df.rename(index=str, columns={\"UTR5_Sequence\": \"utr\"})\n",
    "combined_doudna = seq_df.merge(doudna_df, on=\"GeneName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/doudna_polysome.pkl\", 'wb') as handle:\n",
    "    pickle.dump(combined_doudna, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By isoform: hopefully more high quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  \n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  if sys.path[0] == '':\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "doudna_df = pd.read_csv(\"../Data/TrIP-Seq/doudna_polysome_tripseq_isoform_tpm_ensembl_v75.csv\")\n",
    "doudna_df = doudna_df.rename(index=str, columns={\"gene_id\": \"EnsemblGeneID\",\n",
    "                                                 \"isoform_id\": \"EnsemblTranscriptID\",\n",
    "                                                      \"gene_name\": \"GeneName\"})\n",
    "\n",
    "\n",
    "# replicate 1\n",
    "fractions_1 = doudna_df.select(lambda col: re.match(\"poly._1|80S_1|cyto_1\", col), axis=1)\n",
    "doudna_df[\"count_1\"] = fractions_1.sum(axis=1)\n",
    "doudna_df[\"rl_1\"] = np.sum(np.array(fractions_1) * np.arange(0,9), axis=1)/np.sum(np.array(fractions_1),axis=1)\n",
    "# replicate 2\n",
    "fractions_2 = doudna_df.select(lambda col: re.match(\"poly._2|80S_2|cyto_2\", col), axis=1)\n",
    "doudna_df[\"count_2\"] = fractions_2.sum(axis=1)\n",
    "doudna_df[\"rl_2\"] = np.sum(np.array(fractions_2) * np.arange(0,9), axis=1)/np.sum(np.array(fractions_2),axis=1)\n",
    "# replicate mean\n",
    "fractions = (np.array(fractions_1) + np.array(fractions_2))/2\n",
    "doudna_df[\"count_mean\"] = np.sum(fractions, axis=1)\n",
    "doudna_df[\"rl_mean\"] = np.sum(fractions * np.arange(0,9), axis=1)/np.sum(fractions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df = pd.read_csv(\"../Data/gencodev19_seq.csv\")\n",
    "combined_doudna = seq_df.merge(doudna_df, on=\"EnsemblTranscriptID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/doudna_polysome_iso.pkl\", 'wb') as handle:\n",
    "    pickle.dump(combined_doudna, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_doudna = combined_doudna[combined_doudna[\"count_1\"] + combined_doudna[\"count_2\"] > 2]\n",
    "combined_doudna = combined_doudna.sample(frac=1).reset_index(drop=True)\n",
    "set_vector = [\"test\"]*1500 + [\"val\"]*1500 + [\"train\"]*(len(combined_doudna) - 3000)\n",
    "combined_doudna[\"set\"] = set_vector\n",
    "with open(\"../Data/doudna_polysome_iso_sub.pkl\", 'wb') as handle:\n",
    "    pickle.dump(combined_doudna, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
