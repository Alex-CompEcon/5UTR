{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation notebook\n",
    "Running this is not necessary to replicate the main resultsd & figures, as processed data is provided. This notebook is only there to document the operations used to assemble the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_and_sort(path):\n",
    "    # print(path)\n",
    "    df = pd.read_csv(path)\n",
    "    # drop the suffix\n",
    "    df[\"utr\"] = df[\"utr\"].str[:50]\n",
    "    # reorder\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)  # drop first column\n",
    "    if 'total_reads' in df:\n",
    "        df.sort_values(by=['total_reads'], inplace=True, ascending=False)\n",
    "    else:\n",
    "        df.sort_values(by=['total'], inplace=True, ascending=False)\n",
    "    df.reset_index(inplace=True, drop=True)  # necessary as sorting creates an extra index\n",
    "    return df\n",
    "\n",
    "def extract_fold_data(path):\n",
    "    regex = re.compile(r\".\\d+.\\d+\")\n",
    "    structures = []\n",
    "    energies = []\n",
    "    with open(path, 'r') as fh:\n",
    "        i = 0\n",
    "        for line in fh:\n",
    "            if i % 2 != 0:\n",
    "                structures.append(line.split(\" \")[0])\n",
    "                energies.append(float(regex.findall(line)[0]))\n",
    "            i += 1\n",
    "    return structures, energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We begin by reading in all the raw data, sorting it and removing suffixes not part of the UTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['egfp_unmod_1', 'egfp_pseudo_2', 'egfp_m1pseudo_1', 'egfp_m1pseudo_2', 'mcherry_1', 'mcherry_2', 'designed_library', 'egfp_unmod_2', 'egfp_pseudo_1'])\n",
      "dict_keys(['egfp_unmod_1', 'mcherry_1', 'mcherry_2', 'designed_library', 'egfp_unmod_2'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Read in all Data \"\"\"\n",
    "path = \"../Data/RawData/\"\n",
    "files = [os.path.join(path,file) for file in os.listdir(path) if file.startswith(\"GSM\")]\n",
    "df_list = {re.search(\"_(.*)\\.\",file).group(1):import_data_and_sort(file) for file in files}\n",
    "print(df_list.keys())\n",
    "# Remove the nonstandard chemistries\n",
    "entriesToRemove = ['egfp_pseudo_1', 'egfp_pseudo_2', 'egfp_m1pseudo_1', 'egfp_m1pseudo_2']\n",
    "for k in entriesToRemove:\n",
    "    df_list.pop(k, None)\n",
    "print(df_list.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We subset the data using the same cutoffs as in the sample code (except for the genetic algorithm data, where I impose a cutoff of minimum 200 reads (as I couldnt find which value is actually used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the random mpra data\n",
    "keys = ['egfp_unmod_1', 'egfp_unmod_2', 'mcherry_1', 'mcherry_2']\n",
    "cuts = [280000, 300000, 180000, 170000]\n",
    "for key, cutoff in zip(keys, cuts):\n",
    "    df_list[key] = df_list[key].iloc[:cutoff].copy()\n",
    "\n",
    "# Subsetting the human data    \n",
    "human = df_list[\"designed_library\"][(df_list[\"designed_library\"]['library'] == 'human_utrs') | \n",
    "                                    (df_list[\"designed_library\"]['library'] == 'snv')]\n",
    "human = human.sort_values('total', ascending=False).reset_index(drop=True)\n",
    "human = human.iloc[:25000].copy()\n",
    "\n",
    "# Subsetting the genetic algorithm data\n",
    "GA_types = ['step_random_to_best_allow_uatg',\n",
    " 'step_random_to_best_no_uatgs',\n",
    " 'step_worst_to_best_allow_uatg',\n",
    " 'step_worst_to_best_no_uatg',\n",
    " 'target_allow_uaug_allow_stop',\n",
    " 'target_no_uaug_allow_stop',\n",
    " 'target_no_uaug_no_stop']\n",
    "GA = df_list['designed_library'][df_list['designed_library'][\"library\"].isin(GA_types)]\n",
    "GA = GA.iloc[:sum(GA[\"total\"] >= 200)].copy()\n",
    "\n",
    "df_list.pop(\"designed_library\", None)\n",
    "df_list[\"human\"] = human\n",
    "df_list[\"ga\"] = GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We reduce to the needed columns (utr and rl) and add a library column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_list.items():\n",
    "    df = df.filter(regex=(\"rl|utr\"))\n",
    "    df[\"library\"] = key\n",
    "    df_list[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We prepare the test (20k), val (20k) and train (rest) split for all the sets except ga (only for training) and human (only for validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_list.items():\n",
    "    df[\"set\"] = \"\"\n",
    "    if key == \"human\":\n",
    "        df[\"set\"] = \"test\"\n",
    "    elif key == \"ga\":\n",
    "        df[\"set\"] = \"train\"\n",
    "    else:\n",
    "        df.loc[:20000, \"set\"] = \"test\"\n",
    "        df.loc[20000:40000, \"set\"] = \"val\"\n",
    "        df.loc[40000:, \"set\"] = \"train\"\n",
    "    df_list[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We combine the data into one large frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list.values())    \n",
    "combined_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "egfp_cds = \"atgggcgaattaagtaagggcgaggagctgttcaccggggtg\\\n",
    "gtgcccatcctggtcgagctggacggcgacgtaaacggccacaagttcagcgtgtccggcgagggcgagggcgatgccacctacggca\\\n",
    "agctgaccctgaagttcatctgcaccaccggcaagctgcccgtgccctggcccaccctcgtgaccaccctgacctacggcgtgcagtgctt\\\n",
    "cagccgctaccccgaccacatgaagcagcacgacttcttcaagtccgccatgcccgaaggctacgtccaggagcgcaccatcttcttcaag\\\n",
    "gacgacggcaactacaagacccgcgccgaggtgaagttcgagggcgacaccctggtgaaccgcatcgagctgaagggcatcgacttca\\\n",
    "aggaggacggcaacatcctggggcacaagctggagtacaactacaacagccacaacgtctatatcatggccgacaagcagaagaacgg\\\n",
    "catcaaggtgaacttcaagatccgccacaacatcgaggacggcagcgtgcagctcgccgaccactaccagcagaacacccccatcggc\\\n",
    "gacggccccgtgctgctgcccgacaaccactacctgagcacccagtccaagctgagcaaagaccccaacgagaagcgcgatcacatgg\\\n",
    "tcctgctggagttcgtgaccgccgccgggatcactctcggcatggacgagctgtacaagttcgaataaagctag\".upper()\n",
    "egfp_3utr = \"cgcctcgactgtgccttctagttgccagccatctgttgtttg\".upper()\n",
    "combined_df[\"cds\"] = egfp_cds\n",
    "combined_df[\"3utr\"] = egfp_3utr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We add the energy and the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct, energy = extract_fold_data('../Data/Folding/mpra.energy')\n",
    "combined_df[\"utr_struct\"] = struct\n",
    "combined_df[\"utr_energy\"] = energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We collect the snv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_df = pd.read_csv(\"../Data/SNV/snv_phenotype_log_diff.csv\")\n",
    "snv_df.drop(['Unnamed: 0'], axis=1, inplace=True)  # drop first column\n",
    "snv_df = snv_df[snv_df['obs_diff'] != 0.0]\n",
    "snv_df = snv_df[snv_df['total'] >= 620]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snv_df[\"cds\"] = egfp_cds\n",
    "snv_df[\"3utr\"] = egfp_3utr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We collect the PTR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# We get the PTR data\n",
    "ptr_df = pd.read_csv(\"../Data/PTR/ptr.tsv\", sep='\\t')\n",
    "# We average over all tissues\n",
    "ptr_vals = ptr_df.select(lambda col: col.endswith('PTR'), axis=1).apply(pd.to_numeric, errors='coerce').median(axis=1)\n",
    "ptr_df[\"ptr\"] = ptr_vals\n",
    "\n",
    "# We get the sequences\n",
    "seq_df = pd.read_csv(\"../Data/PTR/seq.tsv\", sep='\\t')\n",
    "\n",
    "# We combine\n",
    "combined_df_ptr = seq_df[[\"GeneName\",\"UTR5_Sequence\", \"CDS_Sequence\", \"UTR3_Sequence\"]].merge(ptr_df)\n",
    "combined_df_ptr = combined_df_ptr.rename(index=str, columns={\"UTR5_Sequence\": \"utr\", \n",
    "                                                            \"CDS_Sequence\":\"cds\",\n",
    "                                                            \"UTR3_Sequence\":\"3utr\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the Riboseq data, compute the load and combine it with sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the sequences\n",
    "seq_df = pd.read_csv(\"../Data/PTR/seq.tsv\", sep='\\t')\n",
    "seq_df = seq_df.rename(index=str, columns={\"UTR5_Sequence\": \"utr\",\n",
    "                                          \"CDS_Sequence\":\"cds\",\n",
    "                                          \"UTR3_Sequence\":\"3utr\"})\n",
    "\n",
    "#We get the andreev riboseq data and combine\n",
    "andreev_df = pd.read_csv(\"../Data/RiboSeq/andreev_counts.tsv\", sep='\\t', decimal=\",\")\n",
    "andreev_df = andreev_df.rename(index=str, columns={\"Gene name\": \"GeneName\", \n",
    "                                                   \"Riboseq control reads, coding\": \"rpf\",\n",
    "                                                   \"RNAseq control, (normalised)\": \"rnaseq_norm\"})\n",
    "\n",
    "andreev_df = andreev_df[(andreev_df[\"rpf\"] > 10) & (andreev_df[\"rnaseq_norm\"] > 10)]\n",
    "andreev_df[\"log_load\"] = np.log(andreev_df[\"rpf\"]/andreev_df[\"rnaseq_norm\"])\n",
    "\n",
    "andreev_merged = andreev_df.merge(seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the xtail pcr3 riboseq data and combine\n",
    "pcr3_df = pd.read_csv(\"../Data/RiboSeq/xtail_counts_pcr3.tsv\", sep='\\t', decimal=\",\")\n",
    "pcr3_df = pcr3_df.rename(index=str, columns={\"Ensembl_ID\": \"EnsemblGeneID\"})\n",
    "pcr3_df = pcr3_df.rename(columns=lambda x: re.sub('.1$','_normalized',x))\n",
    "\n",
    "pcr3_df[\"rpf\"] = (pcr3_df[\"control1(RPF)_normalized\"] + pcr3_df[\"control2(RPF)_normalized\"])/2\n",
    "pcr3_df[\"rna\"] = (pcr3_df[\"control1(mRNA)_normalized\"] + pcr3_df[\"control2(mRNA)_normalized\"])/2\n",
    "pcr3_df = pcr3_df[(pcr3_df[\"rpf\"] > 10) & (pcr3_df[\"rna\"] > 10)]\n",
    "pcr3_df[\"log_load\"] = np.log(pcr3_df[\"rpf\"]/pcr3_df[\"rna\"])\n",
    "\n",
    "pcr3_merged = pcr3_df.merge(seq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the Eichhorn hek293 riboseq data and combine\n",
    "eichhorn_df = pd.read_csv(\"../Data/RiboSeq/Eichhorn_GSE60426_MockHEK293T.tsv\", sep='\\t', decimal=\".\")\n",
    "\n",
    "eichhorn_df[\"log_load\"] = np.log(eichhorn_df[\"RPF_RPKM\"]/eichhorn_df[\"RNA_RPKM\"])\n",
    "\n",
    "eichhorn_merged = eichhorn_df.merge(seq_df)\n",
    "eichhorn_merged = eichhorn_merged.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare more PTR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the wilhelm PTR data\n",
    "wilhelm_ptr_df = pd.read_csv(\"../Data/PTR/wilhelm_ptr.tsv\", sep='\\t', decimal=\",\")\n",
    "wilhelm_ptr_df = wilhelm_ptr_df.dropna()\n",
    "\n",
    "wilhelm_ptr_df = wilhelm_ptr_df.rename(index=str, columns={\"Accessions\": \"EnsemblGeneID\",\n",
    "                                                          \"protein/mRNA ratio\": \"ptr\"})\n",
    "combined_wilhelm = seq_df.merge(wilhelm_ptr_df, on=\"EnsemblGeneID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the polysome profiling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  \n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  if sys.path[0] == '':\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "doudna_df = pd.read_csv(\"../Data/TrIP-Seq/doudna_polysome_tripseq_isoform_tpm_ensembl_v75.csv\")\n",
    "doudna_df = doudna_df.rename(index=str, columns={\"gene_id\": \"EnsemblGeneID\",\n",
    "                                                 \"isoform_id\": \"EnsemblTranscriptID\",\n",
    "                                                      \"gene_name\": \"GeneName\"})\n",
    "\n",
    "\n",
    "# replicate 1\n",
    "fractions_1 = doudna_df.select(lambda col: re.match(\"poly._1|80S_1|cyto_1\", col), axis=1)\n",
    "doudna_df[\"count_1\"] = fractions_1.sum(axis=1)\n",
    "doudna_df[\"rl_1\"] = np.sum(np.array(fractions_1) * np.arange(0,9), axis=1)/np.sum(np.array(fractions_1),axis=1)\n",
    "# replicate 2\n",
    "fractions_2 = doudna_df.select(lambda col: re.match(\"poly._2|80S_2|cyto_2\", col), axis=1)\n",
    "doudna_df[\"count_2\"] = fractions_2.sum(axis=1)\n",
    "doudna_df[\"rl_2\"] = np.sum(np.array(fractions_2) * np.arange(0,9), axis=1)/np.sum(np.array(fractions_2),axis=1)\n",
    "# replicate mean\n",
    "fractions = (np.array(fractions_1) + np.array(fractions_2))/2\n",
    "doudna_df[\"count_mean\"] = np.sum(fractions, axis=1)\n",
    "doudna_df[\"rl_mean\"] = np.sum(fractions * np.arange(0,9), axis=1)/np.sum(fractions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_df = pd.read_csv(\"../Data/gencodev19_seq.csv\")\n",
    "combined_doudna = seq_df.merge(doudna_df, on=\"EnsemblTranscriptID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine transcripts with same 5utr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  after removing the cwd from sys.path.\n",
      "/data/ouga04b/ag_gagneur/home/karollus/.conda/envs/karollus-env/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "doudna_rep1 = combined_doudna[(combined_doudna[\"count_1\"] > 1)]\n",
    "doudna_rep2 = combined_doudna[(combined_doudna[\"count_2\"] > 1)]\n",
    "# replicate 1\n",
    "fractions_1 = doudna_rep1.select(lambda col: re.match(\"poly._1|80S_1|cyto_1|utr\", col), axis=1).groupby(\"utr\").sum()\n",
    "count_1 = fractions_1.sum(axis=1)\n",
    "rl_1 = np.sum(np.array(fractions_1) * np.arange(0,9), axis=1)/np.sum(np.array(fractions_1),axis=1)\n",
    "aggreg_1 = pd.DataFrame({\"utr\":fractions_1.index,\"count_1\":list(count_1),\"rl_1\":rl_1})\n",
    "# replicate 2\n",
    "fractions_2 = doudna_rep2.select(lambda col: re.match(\"poly._2|80S_2|cyto_2|utr\", col), axis=1).groupby(\"utr\").sum()\n",
    "count_2 = fractions_2.sum(axis=1)\n",
    "rl_2 = np.sum(np.array(fractions_2) * np.arange(0,9), axis=1)/np.sum(np.array(fractions_2),axis=1)\n",
    "aggreg_2 = pd.DataFrame({\"utr\":fractions_2.index,\"count_2\":list(count_2),\"rl_2\":rl_2})\n",
    "# merge\n",
    "aggreg = aggreg_1.merge(aggreg_2, on=\"utr\")\n",
    "# mean\n",
    "aggreg[\"rl_mean\"] = (aggreg[\"count_1\"]*aggreg[\"rl_1\"] + aggreg[\"count_2\"]*aggreg[\"rl_2\"])/(aggreg[\"count_1\"] + aggreg[\"count_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all the data to a dict and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"mpra\":combined_df, \"snv\":snv_df, \n",
    "             \"ptr\":combined_df_ptr, \"wilhelm\": combined_wilhelm,\n",
    "             \"andreev\":andreev_merged, \"pcr3\":pcr3_merged, \"eichhorn\": eichhorn_merged,\n",
    "            \"doudna\":aggreg}\n",
    "with open(\"../Data/data_dict.pkl\", 'wb') as handle:\n",
    "    pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add extended data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/data_dict.pkl\", 'rb') as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "varlen_df = pd.read_pickle(\"../Data/RawData/varying_length_25to100_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83919\n",
      "15555\n"
     ]
    }
   ],
   "source": [
    "varlen_df[\"library\"] = varlen_df[\"set\"]\n",
    "## Filter out UTRs with too few less reads\n",
    "varlen_df = varlen_df[varlen_df['total_reads']>=10]\n",
    "print(len(varlen_df[varlen_df['set']=='random']))\n",
    "print(len(varlen_df[varlen_df['set']=='human']))\n",
    "varlen_df.sort_values(['len', 'total_reads'], inplace=True, ascending=False)\n",
    "varlen_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600\n",
      "7600\n"
     ]
    }
   ],
   "source": [
    "varlen_df[\"set\"] = \"train\"\n",
    "for i in range(25,101):\n",
    "    idx = varlen_df[(varlen_df['len']==i) & (varlen_df['library']==\"random\")].iloc[:100].index\n",
    "    varlen_df.loc[idx, \"set\"] = \"test\"\n",
    "    idx_human = varlen_df[(varlen_df['len']==i) & (varlen_df['library']==\"human\")].iloc[:100].index\n",
    "    varlen_df.loc[idx_human, \"set\"] = \"test\"\n",
    "print(len(varlen_df[(varlen_df['library']==\"random\") & (varlen_df['set']==\"test\")]))\n",
    "print(len(varlen_df[(varlen_df['library']==\"human\") & (varlen_df['set']==\"test\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there is no intersection of training and test set\n",
    "set(varlen_df[(varlen_df.set == \"train\") & (varlen_df['library']==\"random\")][\"utr\"]) & set(varlen_df[(varlen_df.set == \"test\") & (varlen_df['library']==\"random\")][\"utr\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to data dict and pickle\n",
    "data_dict[\"varlen_mpra\"] = varlen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/data_dict.pkl\", 'wb') as handle:\n",
    "    pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
